# -*- coding: utf-8 -*-
"""final_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zZVOl-G7JiAnn0FY6TBM5qtTYYNZ4aOO
"""

pip install distance

#installing fuzzywuzzy module
#fuzzywuzzy is  basically used for string matching

pip install fuzzywuzzy

#importing some important libraries
import sys
import os
import warnings
warnings.filterwarnings("ignore")
import distance
import csv
import math
from tqdm import tqdm
from os import path
import numpy as np
import pandas as pd

#reading the tsv file (dataset) of Quora questions
file = pd.read_csv("/content/drive/MyDrive/quora_duplicate_questions.tsv", delimiter="\t")

#checking info and parameters of dataset
file.info()

#Dataset looks like
file.head()

#looking out for the null values in the dataset
void = file.isnull().sum()

#getting info about null values and filling it with 1
print(void)
file.fillna(1)

#info about duplicate and non-duplicate questions
dup_num=file.groupby("is_duplicate")['id'].count().plot.bar()
print(dup_num)

#Getting the number of unique questions in the dataset
qids = pd.Series(file['qid1'].tolist() + file['qid2'].tolist())
unique_ques = len(np.unique(qids))
ques_morethan_onetime = np.sum(qids.value_counts() > 1)
q_vals=qids.value_counts()
print("number of unique questions are: ",q_vals)
q_vals=q_vals.values

import matplotlib.pyplot as plt

import seaborn as sns

#getting the number of questions which are unique and repeated
x = ["questions which are unique" , "Questions which are repeated"]
y = [unique_ques , ques_morethan_onetime]
plt.figure(figsize=(6, 8))
plt.title ("Questions which are repeated and Unique")
sns.barplot(x,y)
plt.show()

#checking in the dataset whether there are any repeated pair of questions
duplicate_pair = file[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()
print ("Number of duplicate questions",(duplicate_pair).shape[0] - file.shape[0])

#info about single question repeated
print ('single question repeated for maximum number of times: {}\n'.format(max(qids.value_counts())))

#before cleanig the dataset, lets calculate some features
#features are then stored in a file called file_fe_without_preprocessing_train.csv
if os.path.isfile('/content/drive/MyDrive/file_fe_without_preprocessing_train.csv'):
  file = pd.read_csv("/content/drive/MyDrive/file_fe_without_preprocessing_train.csv",encoding='latin-1')
else:
    file['freq_qid1'] = file.groupby('qid1')['qid1'].transform('count')
    file['freq_qid2'] = file.groupby('qid2')['qid2'].transform('count')
    file['q1len'] = file['question1'].str.len()
    file['q2len'] = file['question2'].str.len()
    file['q1_n_words'] = file['question1'].apply(lambda row: len(row.split(" ")))
    file['q2_n_words'] = file['question2'].apply(lambda row: len(row.split(" ")))
    def normalized_word_Common(row):
      w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(" ")))
      w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(" ")))
      return 1.0 * len(w1 & w2)
    file['word_Common'] = file.apply(normalized_word_Common, axis=1)
    def normalized_word_Total(row):
      w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(" ")))
      w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(" ")))
      return 1.0 * (len(w1) + len(w2))
    file['word_Total'] = file.apply(normalized_word_Total, axis=1)
    def normalized_word_share(row):
      w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(" ")))
      w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(" ")))
      return 1.0 * len(w1 & w2)/(len(w1) + len(w2))
    file['word_share'] = file.apply(normalized_word_share, axis=1)
    file['freq_q1+q2'] = file['freq_qid1']+file['freq_qid2']
    file['freq_q1-q2'] = abs(file['freq_qid1']-file['freq_qid2'])
    file.to_csv("/content/drive/MyDrive/file_fe_without_preprocessing_train.csv", index=False)
print(file.head())

#understanding the relation between the word_share feature and the questions is_duplicate or not
#plotting the violin graph for it
plt.figure(figsize=(12, 8))

plt.subplot(1,2,1)
sns.violinplot(x = 'is_duplicate', y = 'word_share' , data = file[0:])

plt.subplot(1,2,2)
sns.distplot(file[file['is_duplicate'] == 1.0]['word_share'][0:] , label = "1", color = 'red')
sns.distplot(file[file['is_duplicate'] == 0.0]['word_share'][0:] , label = "0" , color = 'blue' )
plt.show()

#loading the file to get some more extra features
if os.path.isfile('/content/drive/MyDrive/file_fe_without_preprocessing_train.csv'):
  file = pd.read_csv("/content/drive/MyDrive/file_fe_without_preprocessing_train.csv",encoding='latin-1')
  file = file.fillna('')
  print(file)
else:
  print("get file_fe_without_preprocessing_train.csv from drive or run the previous notebook")

import nltk

from nltk import PorterStemmer

#let's get started with preprocessing
#And used beautifulsoap to get data from files
def preprocess(a):
  a = str(a).lower()
  a = a.replace(",000,000", "m").replace(",000", "k").replace("ʹ", "'").replace("’", "'")\
      .replace("won't", "will not").replace("cannot", "can not").replace("can't","can not")\
      .replace("n't", " not").replace("what's", "what is").replace("it's", "it is")\
      .replace("'ve", " have").replace("i'm", "i am").replace("'re", " are")\
      .replace("he's", "he is").replace("she's", "she is").replace("'s", " own")\
      .replace("%", " percent ").replace("₹", " rupee ").replace("$", " dollar ")\
      .replace("€", " euro ").replace("'ll", " will")
  a = re.sub(r"([0-9]+)000000", r"\1m", a)
  a = re.sub(r"([0-9]+)000", r"\1k", a)
  porter = PorterStemmer()
  pattern = re.compile('\W')
  if type(a) == type(''):
    a = re.sub(pattern, ' ', a)
  if type(a) == type(''):
    a = porter.stem(a)
    example1 = BeautifulSoup(a)
    a = example1.get_text()
  return a

#to get stop words, import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
SAFE_DIV = 0.0001
STOP_WORDS = stopwords.words("english")

#using token to split the sentence
#defining the function get_token_features
def get_token_features(q1, q2):
  token_features = [0.0]*10
  # Converting the Sentence into Tokens:
  q1_tokens = q1.split()
  q2_tokens = q2.split()
  if len(q1_tokens) == 0 or len(q2_tokens) == 0:
    return token_features
  # Get the non-stopwords in Questions
  q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])
  q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])
  #Get the stopwords in Questions
  q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])
  q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])
  # Get the common non-stopwords from Question pair
  common_word_count = len(q1_words.intersection(q2_words))
  # Get the common stopwords from Question pair
  common_stop_count = len(q1_stops.intersection(q2_stops))
  # Get the common Tokens from Question pair
  common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))
  token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)
  token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)
  token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)
  token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)
  token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)
  token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)
  # Last word of both question is same or not
  token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])
  # First word of both question is same or not
  token_features[7] = int(q1_tokens[0] == q2_tokens[0])
  token_features[8] = abs(len(q1_tokens) - len(q2_tokens))
  #Average Token Length of both Questions
  token_features[9] = (len(q1_tokens) + len(q2_tokens))/2
  return token_features

#getting the longest common substring
def get_longest_substr_ratio(a, b):
  string1 = list(distance.lcsubstrings(a, b))
  if len(string1) == 0:
    return 0
  else:
    return len(string1[0]) / (min(len(a), len(b)) + 1)

#declaring a function to extract features from each and every question
def extract_features(file):
# each question is preprocessed
  file["question1"] = file["question1"].fillna("").apply(pre_process.preprocess)
  file["question2"] = file["question2"].fillna("").apply(pre_process.preprocess)
  print("token features...")
  # Merging the Features with the dataset
  token_features = file.apply(lambda x: get_token_features(x["question1"], x["question2"]), axis=1)
  file["cwc_min"] = list(map(lambda x: x[0], token_features))
  file["cwc_max"] = list(map(lambda x: x[1], token_features))
  file["csc_min"] = list(map(lambda x: x[2], token_features))
  file["csc_max"] = list(map(lambda x: x[3], token_features))
  file["ctc_min"] = list(map(lambda x: x[4], token_features))
  file["ctc_max"] = list(map(lambda x: x[5], token_features))
  file["last_word_eq"] = list(map(lambda x: x[6], token_features))
  file["first_word_eq"] = list(map(lambda x: x[7], token_features))
  file["abs_len_diff"] = list(map(lambda x: x[8], token_features))
  file["mean_len"] = list(map(lambda x: x[9], token_features))
  #Computing Fuzzy Features on the dataset and then Merging the features with Dataset
  print("fuzzy features..")
  file["token_set_ratio"] = file.apply(lambda x: fuzz.token_set_ratio(x["question1"], x["question2"]), axis=1)
  file["token_sort_ratio"] = file.apply(lambda x: fuzz.token_sort_ratio(x["question1"], x["question2"]), axis=1)
  file["fuzz_ratio"] = file.apply(lambda x: fuzz.QRatio(x["question1"], x["question2"]), axis=1)
  file["fuzz_partial_ratio"] = file.apply(lambda x: fuzz.partial_ratio(x["question1"], x["question2"]), axis=1)
  file["longest_substr_ratio"] = file.apply(lambda x: get_longest_substr_ratio(x["question1"], x["question2"]), axis=1)
  return file

#if nlp features are already created then load it or extract nlp fetures for training purpose
if os.path.isfile('/content/drive/MyDrive/nlp_features_train.csv'):
  file = pd.read_csv("/content/drive/MyDrive/nlp_features_train.csv",encoding='latin-1')
  file.fillna('')
else:
  print("For training purpose getting the features extract")
  file = extract_features(file)
  file.to_csv("/content/drive/MyDrive/nlp_features_train.csv", index=False)
print(file)

duplicate_file = file[file['is_duplicate'] == 1]
not_duplicate_file = file[file['is_duplicate'] == 0]
a = np.dstack([duplicate_file["question1"], duplicate_file["question2"]]).flatten()
b = np.dstack([not_duplicate_file["question1"], not_duplicate_file["question2"]]).flatten()
print ("Number of data points with duplicate pairs) :",len(a))
print ("Number of data points with non duplicate pairs) :",len(b))
#Saving the array into a text file
np.savetxt('train_p.txt', a, delimiter=' ', fmt='%s',encoding="utf-8")
np.savetxt('train_n.txt', b, delimiter=' ', fmt='%s',encoding="utf-8")

from wordcloud import WordCloud, STOPWORDS
from nltk.corpus import stopwords

#removing stopwords from the file
read = path.dirname('.')
textp_w = open(path.join(read, 'train_p.txt')).read()
textn_w = open(path.join(read, 'train_n.txt')).read()
stopwords = set(STOPWORDS)
stopwords.add("br")
stopwords.add("said")
stopwords.remove("not")
stopwords.add(" ")
stopwords.remove("no")
stopwords.remove("like")
print ("Total words in duplicate pair questions :",len(textp_w))
print ("Total words in non duplicate pair questions :",len(textn_w))

#plotting the word cloud to better understand the behavior of dataset
word_c = WordCloud(background_color="white", max_words=len(textp_w), stopwords=stopwords)
word_c.generate(textp_w)
print ("Word Cloud for Duplicate Question pairs")
plt.figure(figsize=(10,10))
plt.imshow(word_c, interpolation='bilinear')
plt.axis("on")
plt.show()

#plotting the word cloud to better understand the behavior of dataset
word_c = WordCloud(background_color="white", max_words=len(textn_w),stopwords=stopwords)
word_c.generate(textn_w)
print ("Word Cloud for non-Duplicate Question pairs:")
plt.figure(figsize=(10,10))
plt.imshow(word_c, interpolation='bilinear')
plt.axis("on")
plt.show()

from sklearn.preprocessing import MinMaxScaler

#Dimentionality reduction using TSNE for Features calculated
sample_file = file[0:5000]
gh = MinMaxScaler().fit_transform(sample_file[['cwc_min',
                                              'cwc_max',
                                              'csc_min',
                                              'csc_max' ,
                                              'ctc_min' ,
                                              'ctc_max' ,
                                              'last_word_eq',
                                              'first_word_eq' ,
                                              'abs_len_diff' ,
                                              'mean_len' ,
                                              'token_set_ratio' ,
                                              'token_sort_ratio' ,
                                              'fuzz_ratio' ,
                                              'fuzz_partial_ratio' ,
                                              'longest_substr_ratio']])
hg = sample_file['is_duplicate'].values

from sklearn.manifold import TSNE

tsne2d = TSNE(
  n_components=2,
  init='random', # pca
  random_state=101,
  method='barnes_hut',
  n_iter=1000,
  verbose=2,
  angle=0.5
).fit_transform(gh)

tsne3d = TSNE(n_components=3,
              init='random', # pca
              random_state=101,
              method='barnes_hut',
              n_iter=1000,
              verbose=2,
              angle=0.5).fit_transform(gh)

#now we will featurize Tfidf weighted word vectors
# avoid decoding problems
file = pd.read_csv("/content/drive/MyDrive/quora_duplicate_questions.tsv", delimiter="\t")
file['question1'] = file['question1'].apply(lambda x: str(x))
file['question2'] = file['question2'].apply(lambda x: str(x))

from sklearn.feature_extraction.text import TfidfVectorizer

# merge texts
questions = list(file['question1']) + list(file['question2'])
tfidf = TfidfVectorizer(lowercase=False, )
tfidf.fit_transform(questions)
# dict key:word and value:tf-idf score
word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))

#train on complete dataset with TFidf features
# Load Basic Features
basic_feature = pd.read_csv("/content/drive/MyDrive/df_fe_without_preprocessing_train.csv",encoding='latin-1')

#getting info about the number of column in dataset with basic feature
print("Columns : ",basic_feature.columns)
print("\nNumber of columns : ",len(basic_feature.columns))

# Load Advance Features
advance_features = pd.read_csv("/content/drive/MyDrive/nlp_features_train.csv",encoding='latin-1')

advance_features.head()

#getting info about the number of column in dataset with advance feature
print("Columns : ",advance_features.columns)
print("\nNumber of columns : ",len(advance_features.columns))

# let's drop some column from basic feature dataset
basic_feature = basic_feature.drop(['qid1','qid2'],axis=1)
# let's drop some column from advance feature dataset
advance_features = advance_features.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)
# add both the dataset after dropping some columns
Total_features = basic_feature.merge(advance_features, on='id',how='left')

#filling up the null values in dataset
null_rows = Total_features[Total_features.isnull().any(1)]

#removing null rows from the dataset
Total_features = Total_features[Total_features['question1'].notnull()]
Total_features = Total_features[Total_features['question2'].notnull()]
nan_rows = Total_features[Total_features.isnull().any(1)]

#lets get the info aboout our new dataset with all the features
Total_features.info()

#now separating the target feature from the dataset
target_feature = Total_features['is_duplicate']

Total_features.drop(['id','is_duplicate'], axis=1, inplace=True)

#Lets perform TF-IDF Tokenization on two columns 'question1' and 'question2'

# Lets create Tfidf Vectorizer on question1 column of the dataset
tfidfVectorizer_q1 = TfidfVectorizer()
Token_question1 = tfidfVectorizer_q1.fit_transform(Total_features['question1'].values.astype('U'))

# Lets create Tfidf Vectorizer on question2 column of the dataset
tfidfVectorizer_q2 = TfidfVectorizer()
Token_question2 = tfidfVectorizer_q2.fit_transform(Total_features['question2'].values.astype('U'))

from scipy.sparse import hstack

# now let us combine all features of Token_question1 and Token_question2
q1_q2 = hstack((Token_question1,Token_question2))

# Terminating unnecessary columns
Total_features.drop(['question1','question2'], axis=1, inplace=True)

# Combining all of the  basic, advance and tfidf features
Final_features = hstack((Total_features, q1_q2),format="csr",dtype='float64')

# info about the Final_features
Final_features.shape

from sklearn.model_selection import train_test_split

#splitting the dataset into 70:30 ratio
x_train,x_test, y_train, y_test = train_test_split(Final_features, target_feature, stratify=target_feature, test_size=0.3)

pip install mlxtend

pip install six

from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, log_loss
#from mlxtend.classifier import StackingClassifier

#Applying Logistic Regression on the dataset
alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.
log_error_array=[]
for i in alpha:
  clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)
  clf.fit(x_train, y_train)
  sig_clf = CalibratedClassifierCV(clf, method="sigmoid")
  sig_clf.fit(x_train, y_train)
  predict_y = sig_clf.predict_proba(x_test)
  log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))
  print('For alpha = ', i, "The log loss is:",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))
fig, ax = plt.subplots()
ax.plot(alpha, log_error_array,c='g')
for i, txt in enumerate(np.round(log_error_array,3)):
  ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))
plt.grid()
plt.title("(Cross Validation Error)")
plt.xlabel("(Value of Alpha)")
plt.ylabel("(Error)")
plt.show()
best_alpha = np.argmin(log_error_array)
clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)
clf.fit(x_train, y_train)
sig_clf = CalibratedClassifierCV(clf, method="sigmoid")
sig_clf.fit(x_train, y_train)
predict_y = sig_clf.predict_proba(x_train)
print('For the best alpha among all = ', alpha[best_alpha], "The value of log loss for train is:",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))
predict_y = sig_clf.predict_proba(x_test)
print('For the best alpha among all = ', alpha[best_alpha], "The value of log loss for test is:",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))
predicted_y =np.argmax(predict_y,axis=1)
print("Total number of data points :", len(predicted_y))

#Applying Linear SVM on the dataset
alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.
log_error_array=[]
for i in alpha:
  clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)
  clf.fit(x_train, y_train)
  sig_clf = CalibratedClassifierCV(clf, method="sigmoid")
  sig_clf.fit(x_train, y_train)
  predict_y = sig_clf.predict_proba(x_test)
  log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))
  print('For alpha = ', i, "The log loss is:",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))
fig, ax = plt.subplots()
ax.plot(alpha, log_error_array,c='g')
for i, txt in enumerate(np.round(log_error_array,3)):
  ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))
plt.grid()
plt.title("(Cross Validation Error)")
plt.xlabel("(Alpha)")
plt.ylabel("(Error)")
plt.show()
best_alpha = np.argmin(log_error_array)
clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)
clf.fit(x_train, y_train)
sig_clf = CalibratedClassifierCV(clf, method="sigmoid")
sig_clf.fit(x_train, y_train)
predict_y = sig_clf.predict_proba(x_train)
print('For the best alpha among all = ', alpha[best_alpha], "The value of log loss for train is:",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))
predict_y = sig_clf.predict_proba(x_test)
print('For the best alpha among all = ', alpha[best_alpha], "The value of log loss for train is:",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))
predicted_y =np.argmax(predict_y,axis=1)
print("Total number of data points :", len(predicted_y))

#now let us calculate vectors from the trained GloVe model on wikipedia
import spacy

import en_core_web_sm

# en_vectors_web_lg contains around one million unique vectors.
nlp = en_core_web_sm.load()

vecs1 = []

#to let know about the progress, we used tqdm
for qu1 in tqdm(list(file['question1'])):
    doc1 = nlp(qu1) 
    # number of dimensions are 384
    mean_vec1 = np.zeros([len(doc1), 384])
    for word1 in doc1:
        #  calculating word2vec
        vec1 = word1.vector
        # fetching the df score
        try:
            idf = word2tfidf[str(word1)]
        except:
            idf = 0
        # computing the final vector and reshaping the mean_vec1
        mean_vec1=np.arange(384).reshape(4,96)
        tuv= vec1 * idf
        mean_vec1 = mean_vec1 + tuv
    mean_vec1 = mean_vec1.mean(axis=0)
    vecs1.append(mean_vec1)
file['q1_feats_m'] = list(vecs1)

vecs2 = []

#to let know about the progress, we used tqdm
for qu2 in tqdm(list(file['question2'])):
  doc2 = nlp(qu2)

  # number of dimensions are 384
  mean_vec2 = np.zeros([len(doc2), 384])
  for word2 in doc2:
    #  calculating word2vec
    vec2 = word2.vector
    # fetching the df score
    try:
      idf = word2tfidf[str(word2)]
    except:
      idf = 0
    # computing the final vector and reshaping the mean_vec2
    mean_vec2=np.arange(384).reshape(4,96)
    xyz = vec2 * idf
    mean_vec2 = mean_vec2 + xyz
  mean_vec2 = mean_vec2.mean(axis=0)
  vecs2.append(mean_vec2)
file['q2_feats_m'] = list(vecs2)

#now let us preprocess the nlp_feature file for XgBoost model
if os.path.isfile('/content/drive/MyDrive/nlp_features_train.csv'):
  dfnlp = pd.read_csv("/content/drive/MyDrive/nlp_features_train.csv",encoding='latin-1')
else:
  print("chech your drive, it should be there OR provide proper path")
if os.path.isfile('/content/drive/MyDrive/file_fe_without_preprocessing_train.csv'):
  dfppro = pd.read_csv("/content/drive/MyDrive/file_fe_without_preprocessing_train.csv",encoding='latin-1')
else:
  print("chech your drive, it should be there OR provide proper path")

df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)
df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)
df3 = file.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)
df3_q1 = pd.DataFrame(df3['q1_feats_m'].values.tolist(), index= df3.index)
df3_q2 = pd.DataFrame(df3['q2_feats_m'].values.tolist(), index= df3.index)

# After preprocessing all the features let us store it into the csv file
if not os.path.isfile('final_features.csv'):
  df3_q1['id']=df1['id']
  df3_q2['id']=df1['id']
  df1 = df1.merge(df2, on='id',how='left')
  df2 = df3_q1.merge(df3_q2, on='id',how='left')
  result = df1.merge(df2, on='id',how='left')
  result.to_csv('final_features.csv')

#let us split the data into train and test for XgBoost model
x_train,x_test, y_train, y_test = train_test_split(final_features, target_feature, stratify=target_feature, test_size=0.3)

#now let us implement the XgBoost model
#import some main libraries for XgBoost models
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV,StratifiedK

feature_train, feature_test, target_train, target_test = train_test_split(advance_features, y_true, stratify=y_true, test_size=0.3)

#let us use some of the values for our model and define hyperparameter tuning
number_of_estimators = [100, 300, 500, 700, 900, 1100, 1300, 1500]
rate_of_learning = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
colsample_bytree = [0.1, 0.3, 0.5, 0.7 , 0.9, 1]
sub_sample = [0.1,0.3,0.5,0.7,0.9,1]

#defining hyperparameter tuning for XgBoost model
def hyperparameter_tunning(X,Y):
    param_grid = dict(rate_of_learning=rate_of_learning,
                      number_of_estimators=number_of_estimators,
                      colsample_bytree = colsample_bytree,
                      sub_sample = sub_sample)
    
    XG_model = XGBClassifier(nthread=-1)
    kfold = StratifiedKFold(n_splits=5, shuffle=True)
    random_search = RandomizedSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
    random_result = random_search.fit(X,Y)
    
    #printing the best results and analyzing it
    print("Best: %f using %s" % (random_result.best_score_, random_result.best_params_))
    print()
    means = random_result.cv_results_['mean_test_score']
    stds = random_result.cv_results_['std_test_score']
    params = random_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))
        
    return random_result

xGBClassifier = XGBClassifier(max_depth=3, 
                              learning_rate=0.1, 
                              n_estimators=1100,
                              subsample=0.3,
                              colsample_bytree= 0.7, 
                              nthread=-1)
xGBClassifier

#defining all the parameters before evaluating the model
params = {}
params['objective'] = 'binary:logistic'
params['eval_metric'] = 'logloss'
params['eta'] = 0.02
params['max_depth'] = 3
params['colsample_bytree'] = 0.7
params['n_estimators'] = 1100
params['subsample'] = 0.3
params['learning_rate'] = 0.1
params['nthread'] = -1
params['silent'] = 1

#creating the test and train file for the model
docu_train = xgb.DMatrix(feature_train, label=target_train)
docu_test = xgb.DMatrix(feature_test, label=target_test)

watchlist = [(docu_train, 'train'), (docu_test, 'valid')]

bst = xgb.train(params, docu_train, 400, watchlist,verbose_eval= False,early_stopping_rounds=20)

xgdmat = xgb.DMatrix(feature_train,target_train)
predict_y = bst.predict(docu_test)
print("The test log loss is:",log_loss(target_test, predict_y, labels=clf.classes_, eps=1e-15))